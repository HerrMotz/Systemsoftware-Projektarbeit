
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%

% Daniel Motz's packages
\usepackage{amsfonts}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{minted}
% END Daniel Motz's packages

\begin{document}
%
\title{Scheduling on Multicore Systems}
%
%\titlerunning{Abbreviated paper title}
%
\author{Friedrich Answin Daniel Motz}
%
\authorrunning{D. Motz}
%
\institute{FMI, Friedrich-Schiller-University, Jena, Germany\\
Seminar Work for Systemsoftware\\
\email{daniel.motz@uni-jena.de}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
This seminar work contains insights into memory architecture in multi-core systems, multi-core scheduling concepts, such as global queue and distributed queue, multi-core scheduling on Linux and introduces other hardware features, like hardware multithreading, to the reader. It explains the importance of cache / hardware affinity and introduces methods, by which it can be made possible.

\keywords{Systemsoftware \and multicore \and scheduling.}
\end{abstract}
%
%
%
\section{Introduction}
Processor Scheduling is, by definition, the algorithm which holds it all together. It determines which
process is permitted to run, when it may run, and for how long - it is ruler \textit{dei gratia}
and the Computer Architects are it's gods. Some of these gods have left us great works of art,
like the CFS Scheduler (current Linux scheduler) or $O(1)$ Scheduler (previous Linux scheduler), 
but some behaviours of mentioned schedulers seem erratic. New research points to scheduling prediction 
based on modelling of CPU schedulers' behaviours~\cite{meehean}.

Moore's law will not be here to protect us from what is inevitable: transistor count will stop
doubling. At some point, transistor sizes will meet atomic boundaries, at which point growth can
solely be realised by parallelisation. It's questionable when Quantum Computers
will replace classic computers in scientific calculations, but it might not take so long. For
now, one may safely assume that computation will need to rely on classic CPU schedulers for some
years to come, especially in desktop computers. So, there is an undisputed necessity to efficiently
manage multicore CPUs, not only in High Performance Computing, but also desktop PCs.~\cite{thread-sched-mc-platforms}

The key challenges for multicore scheduling, are to achieve the goals formerly set for single-core
CPU schedulers, meaning, that an octa-core processor should behave to a programme, as though it 
were a single-core processor with an octupled performance. With faster CPUs - and RAM clock speeds not
keeping up, cycle times to fetch from memory become higher and lie in the hundreds.~\cite{fedorova-phd}
Making use of on-chip caches (all levels of them) and other hardware features is therefore key.
Otherwise, overhead introduced by cache misses will limit the possible effectiveness of multicore
CPUs. Cache-affinity, or more generally, hardware affinity is a sub-goal of scalability.

Research by Intel from 2007 suggests, that using a runtime called McRT, might enable near linear
scalability for certain tasks, such as recognition, mining and synthesis applications or Xvid.~
\cite{scalability-of-programs-multi-cores}

\section{Multi-Core Processor Architecutre}
A multi-core processor may be any processor with several cores.~\cite{multicore-slides}

There are two types of multiprocessor memory types:
shared memory and distributed memory.

Shared memory systems have one global memory space, in which each core can access any data in the
memory.~\cite{multicore-slides}

Distributed memory means, that each core has a local or private memory. Copying data from one local
memory to another, will require a request The local or private address space of each processor cannot be addressed by other processors.~\cite{multicore-slides}

There are advantages to both, e.g. should a system use shared memory and private caches, the issue
of cache validity occurs. Should a core write data to memory (or it's cache, depending on whether
it is write-through cache or something else), it would make data in another core's cache
invalid (meaning the value is older). The cache controller of this processor will then need to 
\textit{snoop} the memory \textit{bus}, meaning it monitors whether a memory address it holds in cache has been written to, invalidate it's cache and fetch it from memory - when needed.~\cite{ostep} This is known as cache coherence.~\cite{multicore-slides}

On the other hand, should there only be shared cache on every level, the time to retrieve data for a
processor would be longer, compared to private cache. Therefore, plenty of research has been done to determine at which level cache should be shared or 
private, e.g. Armv8-A has 3 cache layers, two of which are internal and one shared level-3 cache.~\cite{arm-docs-cache}

Also introduced by the architecture, is hardware affinity. A process might run on one specific core. It will load instructions and data, which the cache controller will kindly remember for later use. A CPU interrupt lets another process run and for some reason, the scheduler decides to run the previous process on a different core (with a different cache). The cache controller will have to newly fetch from the memory, using up valuable cycles. As discussed before, it may take some tens or hundred cycles to fetch from memory once. There are schedulers, which then let another process run, while I/O-actions or memory fetches are run in the background, however, this could be avoided in the first place, by always (or preferably) running a process on the same processor, which in turn needs to be kept in mind by the scheduler.~\cite{ostep}

\subsection{UMA and NUMA}
Non-Uniform Memory Access or NUMA gives software developers and applications insight into the distance between memory banks and processors.~\cite{numa-policies-bolosky} The opposite would be Uniform Memory Access or UMA, also called Symmetric Multiprocessing or SMP, where all memory cells are connected to the mainboard chipset's Northbridge. This means, that the distance between each core and memory cell is the same. In multi-core systems with one CPU socket, the largest distance between a memory cell and a core is relatively small, compared to multi-socket computers. NUMA-nodes therefore help to enable hardware affinity on multi-socket systems, which is a goal of a multicore scheduler. Multi-socket systems have become a common occurrence, even in low-performance servers (or some power-users' workstations) at the time of writing. Frameworks for task parallelisation, such as the Task Parallel Library~\cite{tpl-microsoft} are hindered on NUMA systems. Using algorithms, this effect may be limited and performance increased by a factor of 5.~\cite{scalable-numa} This relatively new research shows, that there is room for improvement on NUMA.

NUMA technically is a shared-memory system, as described above. Other sockets, correctly NUMA-nodes, can address the so-called "local" memory-bank of another NUMA-node. Due to the transparency to the operating system regarding NUMA-node-structure, the software can prioritise moving tasks to cores in the same NUMA-node. Moving tasks to other NUMA-nodes is possible, however, depending on implementation, it would require to copy the memory's contents to the other NUMA-node as well. This is in addition to having to re-cache contents on the new core.

Most importantly, NUMA has a big advantage to UMA. UMA has equal response time for any memory access from any CPU, with no locality advantage whatsoever. An important improvement of NUMA compared to UMA, is increased aggregated memory bandwidth, because each CPU has its own memory controller.~\cite{hpc-numa-slides}

\section{Multi-Core Processor Scheduling}
CPU scheduling on single-core processors is already discussed in the lecture and are not the main focus of this seminar work on multi-core processor scheduling. It is therefore omitted.

\subsection{Global Queue}
When using Single-Queue-Scheduling~\cite{ostep} or Global Queue Architecture~\cite{meehean} the scheduler has one global queue, from which it fetches a task to run. This concept only describes the accessibility for the queue, meaning it says nothing about how to decide which task shall be ran.

The major advantage is that a core can access all tasks. We can also easily set the probability that a task is chosen. In a one-queue-per-core architecture, this would only be possible if we were to regularly monitor each queue and reassign tasks, e.g. from full to empty queues.

The major drawback of a global run queue is synchronisation between cores. This introduces overhead due to locking.~\cite{meehean}

\subsection{Distributed Queue}
Using Distributed-Queue-Architecture means that each core has a private queue. This necessitates a mechanism, which assigns a task to a queue and balances the queues to distribute the load evenly. As is well known, tasks run unpredictable. The scheduler has no knowledge about when a task is going to finish (or maybe crash). 

The intervals at which we check for imbalances are therefore the distributed queue's disadvantage. Another effect is, that imposing a scheduling policy is impossible. It would require to, again, maintain a data structure shared between cores, which manages the task to run next - this is against this architecture's definition.

There are two immense advantages over global queues: the distributed queue conceptually enabled cache affinity. A task will only periodically be switched to another core.

However, compared to a global queue, there is more effort to maintain a distributed queue, which does not suffer from locking or locking at scale (e.g. when there are hundreds of cores accessing the same queue).~\cite{ostep,meehean}

\subsection{Linux Schedulers}
The Linux Scheduler (at the time of writing) is the Completely Fair Scheduler (CFS). Formerly, Linux used the $O(1)$ Scheduler. 

\subsubsection{The $O(1)$ Scheduler}

maintains two arrays; one contains the active tasks (queued to be run) and the other all expired tasks (these processes have run for long enough, an arbitrary but well-chosen time-frame or quantum is set, after which a task becomes expired). Now at some point, all tasks will have run for the said quantum and are now expired. The scheduler then simply reassigns the active queue to be the expired one and vice versa.~\cite{linux-journal-cfs,ostep,meehean} 

Firstly, $O(1)$ is a time-sharing scheduler. The concept of time-sharing developed out of the need to enable many users users to work on one computer - which reduced costs per user by a great deal.~\cite{wikipedia-timesharing-history} 
$O(1)$ differentiates between \textit{real-time} and \textit{normal} priorities. Tasks assigned a real-time priority are always scheduled before a normal-priority task. In addition, it uses a similar mechanism to Multilevel-Feedback Queue architecture (MLFQ)~\cite{ostep} for \textbf{normal-priority tasks}, called decay-usage mechanism.~\cite{epema} Like MLFQ a task with higher priority preempts tasks with lower priority and are interrupted at set timeslices. Unlike MLFQ, decay-usage assigns larger time-slices to interactive tasks. These slices range from 5 to 800ms~\cite{meehean}. Depending on which resources a task uses, it is assigned an interactivity index. Resources like the terminal yield a higher interactivity, than e.g. reading from disk.

$O(1)$ uses the already mentioned distributed run queue. Now, it has already been mentioned, how distributed run queues suffer from overhead due to the necessity of balancing tasks. $O(1)$ does this 

\subsubsection{The Completely Fair Scheduler (CFS)} aims to divide up processing power \textit{completely fairly}. Keeping in mind the goals set earlier, that single-core scheduling policies should transfer to multi-core processors, we now have a solution. According to the CFS' author, each task has a \emph{wait\_runtime} property. It accounts (in nanoseconds) how much time has elapsed, while another process has run. When the time comes to select another task to run, CFS selects the task with the highest \emph{wait\_runtime} or in other words - "the task with the 'gravest need' for more CPU time"~\cite{molnar-on-cfs-redhat-forum}.~\cite{molnar-on-cfs-redhat-forum,linux-journal-cfs,ostep,meehean} 

The Law of Large Numbers tells us, that if we do an experiment often enough, the absolute probability will eventually converge to the theoretical probability. In effect, Molnar created a scheduler that works by these principles. Clearly, a process might exit before it will have gotten a fair share of processing power, but it will not starve either.

\subsection{Scheduling Domains}
Linux introduces scheduling domains and scheduling groups to hierarchically organise balancing between CPU-cores. There is a top-level scheduling domain, which has children scheduling domains, which in turn contains a subset of the CPUs.
This enables the Linux Kernel to prioritise balancing between nodes, differentiating between physical nodes and avoids moving a task, e.g. from one NUMA-node to another arbitrarily. For this to happen, a high disparity in load would have had to occur.~\cite{understanding-linux}

The development behind scheduling domains sought to increase efficiency with hyperthreading processors and NUMA architectures.~\cite{lwn-kernel-dev}

\subsection{Hardware Multithreading}
This topic is related to context-switching decisions related to specific events, made by the hardware.
A multithreaded processor may run a task which accesses I/O (or any other resource other than cache), causing to \textit{stall} the processor, which would usually wait for the maybe hundreds of cycles. A \textit{coarse-grained} multithreaded processor would switch to a different process while the high-latency operation fetches from said resource. This causes the processor to idle for a bit and then switch to a new task when the response might soon return.~\cite{fedorova-phd}
Fine-grained multithreaded processors switches every cycle, to avoid this behaviour and possible waste of cycles.~\cite{fedorova-phd}

\subsubsection{Acknowledgements} Please place your acknowledgments at
the end of the paper, preceded by an unnumbered run-in heading (i.e.
3rd-level heading).

%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
 
\newpage

\begin{thebibliography}{8}

\bibitem{ostep}
ARPACI-DUSSEAU, R. H., ARPACI-DUSSEAU, A. C.: Operating Systems Three Easy Pieces. Version 0.80. Arpaci-Dusseau Books, Inc.,
University of Wisconsin-Madison (2014)

\bibitem{meehean}
MEEHEAN, J. T., : Towards Transparent CPU Scheduling. Dissertation,
University of Wisconsin-Madison (2011)

\bibitem{syssoft}
FLEISCHAUER, M., MITTERREITER, M., SICKERT, S., Systemsoftware.
University of Jena (2022)

\bibitem{understanding-linux}
Daniel Bovet, and Marco Cesati. Understanding the Linux Kernel. O’Reilly
Media, Inc., 3rd edition, 2005.

\bibitem{epema}
D. H. J. Epema. 1998. Decay-usage scheduling in multiprocessors. ACM Trans. Comput. Syst. 16, 4 (Nov. 1998), 367–415. DOI:https://doi.org/10.1145/292523.292535

\bibitem{thread-sched-mc-platforms}
RAJAGOPANAL, M., LEWIS, B. T., ANDERSON, T. A.: Thread Scheduling for Multi-Core Platforms. Programming Systems Lab, Intel Corporation (2007)

\bibitem{scalability-of-programs-multi-cores}
SAHA, B., ADL-TABATABAI, A., GHULOUM, A., RAJAGOPALAN,
M., HUDSON, R., PETERSEN, L., MENON, V.,
MURPHY, B., SHPEISMAN, T., SPRANGLE, E., ROHILLAH, A.,
CARMEAN, D., AND FANG, J. Enabling scalability and performance
in a large scale CMP environment. EuroSys (March 2007)

\bibitem{fedorova-phd}
FEDOROVA, A., Operating System Scheduling for Chip Multithreaded
Processors. PhD thesis, Division of Engineering and
Applied Sciences, Harvard University, Cambridge, MA, Nov
2006.

\bibitem{numa-policies-bolosky}
BOLOSKY, W. J., SCOTT, M. L., FITZGERALD, R. P., FOWLER, R. J., and Cox, A. L.
NUMA policies and their relation to memory
architecture. April 1991.

\bibitem{scalable-numa}
DREBES, A., POP, A., HEYDEMANN, K., COHEN, A., DRACH, N.
Scalable Task Parallelism for NUMA: A Uniform Abstraction
for Coordinated Scheduling and Memory Management. September 2016.

\bibitem{tpl-microsoft}
Microsoft Docs on Task Parallel Library, \url{https://docs.microsoft.com/de-de/dotnet/standard/parallel-programming/task-parallel-library-tpl}. 
Last accessed 13 Apr 2022

\bibitem{multicore-slides}
Multi-core architectures. Presentation. \url{https://www.cs.cmu.edu/~fp/courses/15213-s07/lectures/27-multicore.pdf}. 
Last accessed 13 Apr 2022

\bibitem{arm-docs-branch-prediction}
arm Developer documentation. Branch prediction. \url{https://developer.arm.com/documentation/ddi0338/g/prefetch-unit/branch-prediction}
Last accessed 13 Apr 2022

\bibitem{arm-docs-neon-simd-mimd}
NEON Programmer's Guide. Docs. \url{https://developer.arm.com/documentation/den0018/a/?lang=en}
Last accessed 13 Apr 2022

\bibitem{intel-docs-intrinsics-simd-mimd}
Intel(R) Intrinsics Guide. Docs. \url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}
Last accessed 02 Apr 2022

\bibitem{arm-docs-cache}
Arm Docs. Armv8 Caches. \url{https://developer.arm.com/documentation/den0024/a/Caches}
Last accessed 13 Apr 2022

\bibitem{tlp-smt-cmp-umd-edu}
Thread Level Parallelism Lecture, OWENS, J. \url{https://www.nvidia.com/content/cudazone/cudau/courses/ucdavis/lectures/tlp1.pdf}
Last accessed 08 April 2022

\bibitem{kernel-org-cfs}
The Linux Kernel Archives. Linux Scheduler > Completely Fair Scheduler. \url{https://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html}
Last accesses 13 Apr 2022

\bibitem{linux-journal-cfs}
Linux Journal. Completely Fair Scheduler. PABLA, C. S., August 2009.
Last accessed 13 Apr 2022

\bibitem{molnar-on-cfs-redhat-forum}
This is the CFS scheduler. MOLNAR, I. \url{https://people.redhat.com/mingo/cfs-scheduler/sched-design-CFS.txt}
Last accessed 13 Apr 2022

\bibitem{hpc-numa-slides}
EDMOND. HPC-NUMA. Lecture Slides. \url{https://faculty.cc.gatech.edu/~echow/ipcc/hpc-course/HPC-numa.pdf}
Georgia Tech University (2016)
Last accessed 13 Apr 2022

\bibitem{lwn-kernel-dev}
Scheduling domains. user: corbet.\url{https://lwn.net/Articles/80911/}
LWN.net (2004)
Last accessed 13 Apr 2022

\bibitem{wikipedia-timesharing-history}
Wikipedia. Time-sharing. \url{https://en.wikipedia.org/wiki/Time-sharing}
Last accessed 14 Apr 2022

\end{thebibliography}

\newpage

\subsubsection{Selbstständigkeitserklärung}
\noindent Ich versichere, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen 
als die angegebenen Hilfsmittel und Quellen benutzt habe. Zitate und gedankliche Übernahmen 
aus fremden Quellen (einschließlich elektronischer Quellen) habe ich kenntlich gemacht. 
Die eingereichte Arbeit wurde bisher keiner anderen Prüfungsbehörde vorgelegt und wurde auch
nicht veröffentlicht. Mir ist bekannt, dass eine unwahre Erklärung rechtliche Folgen haben 
und insbesondere dazu führen kann, dass die Arbeit als nicht bestanden bewertet wird.

\vspace{10mm}

\noindent
\textbf{Jena, den 14.04.2022}
\hfill
Daniel Motz

\par\noindent\rule{\textwidth}{0.4pt}

\end{document}

